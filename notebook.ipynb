{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fraud Prediction Using Skewed Data\n","\n","### How do we achieve good predictive power in the statistical model? Can sampling techniques help? Let's find out!\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Setup\n","To prepare your environment, you need to install some packages.\n","\n","### 1.1 Install the necessary packages\n","\n","You need the latest versions of these packages:<br>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Read the Data & convert it into Dataframe\n","Click on Insert to code and then select Insert pandas DataFrame in the below empty cell."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import os, types\n","import pandas as pd\n","from botocore.client import Config\n","import ibm_boto3\n","\n","def __iter__(self): return 0\n","\n","# @hidden_cell\n","# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n","# You might want to remove those credentials before you share the notebook.\n","client_064a0f5bbb034a4e93c3166749a53d98 = ibm_boto3.client(service_name='s3',\n","    ibm_api_key_id='',\n","    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n","    config=Config(signature_version='oauth'),\n","    endpoint_url='https://s3.private.eu.cloud-object-storage.appdomain.cloud')\n","\n","body = client_064a0f5bbb034a4e93c3166749a53d98.get_object(Bucket='predictfraud-donotdelete-pr-ozcvqw2emrjwfb',Key='creditcard.csv')['Body']\n","# add missing __iter__ method, so pandas accepts body as file-like object\n","if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n","\n","df_data_2 = pd.read_csv(body)\n","df_data_2.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Assign a new name for the dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Rename the dataframe to df'''\n","\n","df = df_data_2"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Descriptive statistics on the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Time\")\n","print(df.Time.describe())\n","print(\"V1\")\n","print(df.Time.describe())\n","print(\"Amount\")\n","print(df.Amount.describe())\n","print(\"Class\")\n","print(df.Class.describe())\n","print(df.Class.nunique())\n","print(\"ALL\")\n","print(df.describe())"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Use Histogram to visualize"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.Time.hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.Class.hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['Amount'].hist(by=df['Class'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df.groupby('Class').size())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.hist(figsize=(15,15))"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Check the spread of fraud vs non-fraud on selected variables\n","#### This is done to identify variables selection which has good spread of frauds and non frauds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample = df[df['Class']==0]\n","fraud = df[df['Class']==1]\n","print('V9 - V10')\n","plt.scatter(fraud['V9'], fraud['V10'],s=1, color='r')\n","plt.scatter(sample['V9'], sample['V10'], s=1, color='g')\n","plt.show()\n","plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('V16-V17')\n","plt.scatter(sample['V16'], sample['V17'], s=1, color = 'g')\n","plt.scatter(fraud['V16'], fraud['V17'], s=1, color = 'r')\n","plt.show()\n","plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('V17 - V18')\n","plt.scatter(sample['V18'], sample['V17'], s=1, color = 'g')\n","plt.scatter(fraud['V18'], fraud['V17'], s=1, color = 'r')\n","plt.show()\n","plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('V1 - V3')\n","plt.scatter(sample['V1'], sample['V3'], s=1, color = 'g')\n","plt.scatter(fraud['V1'], fraud['V3'], s=1, color = 'r')\n","plt.show()\n","plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('V1 - V2')\n","plt.scatter(sample['V1'], sample['V2'], s=1, color = 'g')\n","plt.scatter(fraud['V1'], fraud['V2'], s=1, color = 'r')\n","plt.show()\n","plt.clf()"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Split the data into train & test data sets using 70:30 mix\n","#### The model will be built on training data and will be applied on the test data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split The Data with all variables\n","\n","from sklearn.model_selection import train_test_split\n","\n","x = df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n","       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n","       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\n","y = df['Class']\n","\n","xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.30, random_state=0)\n","print('xtrain shape')\n","print(xtrain.shape)\n","print('xtest shape')\n","print(xtest.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## 8. Use Random Forest Algorithm\n","#### A brief about Random Forest Algorithm\n","Random forest classifier creates a set of decision trees from randomly selected subset of training set. It then aggregates the votes from different decision trees to decide the final class of the test object.  Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests will avoid overfitting which will enhance the accuracy of the model on new data. This is a Bagging based algorithm which is used for reducing Overfitting in order to create strong learners for generating accurate predictions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# RF Classifier explained\n","\n","'''rf = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=4)\n","n_estimators : integer, optional (default=10)\n","The number of trees in the forest.\n","oob_score : bool (default=False)\n","Whether to use out-of-bag samples to estimate the generalization accuracy.\n","n_jobs : integer, optional (default=1)\n","The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.\n","'''\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","\n","rfmodel = RandomForestClassifier()\n","rfmodel.fit(xtrain,ytrain)\n","print('model')\n","print(rfmodel)\n","\n","ypredrf = rfmodel.predict(xtest)\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, ypredrf))\n","print('classification report')\n","print(metrics.classification_report(ytest, ypredrf))\n","print('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))"]},{"cell_type":"markdown","metadata":{},"source":["#### In the above classification report we are getting the f1-score of 89 which is the harmonic mean of precision & recall scores.\n","##### Recall is where the model tries to recollect the number of instances, in this case the model has been able to recollect 82% of frauds and able to classify them as frauds 96 % of the recollected events which is the precision of the model. \n","##### Area under the curve signifies the accuracy of the model (values between 0 & 1) where the score towards 1 indicate high predictive power.\n","# <font color='Red'> Note  : The numbers for recall, precision, F1 Score & Area under the curve can change for different runs of the model due to stocastic nature of the algorithms."]},{"cell_type":"markdown","metadata":{},"source":["# 9. Use Gradient Boosting Alogrithm\n","#### A Brief about Gradient Boosting Algorithm\n","Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function(a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event). This is a Boosting based algorithm which is an ensemble technique to combine weak learners to create a strong learner that can make accurate predictions. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# GBM Classifier explained\n","\n","'''params = {'n_estimators': 500, 'max_depth': 3, 'subsample': 0.5, 'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n","clf = ensemble. GradientBoostingClassifier(**params)\n","n_estimators : int (default=100)\n","The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting, so a large number usually results in better performance.\n","max_depth: integer, optional (default=3)\n","maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. The best value depends on the interaction of the input variables.\n","subsample: float, optional (default=1.0)\n","The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n","random_state : int, RandomState instance or None, optional (default=None)\n","If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n","learning_rate : float, optional (default=0.1)\n","learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n","min_samples_leaf : int, float, optional (default=1)\n","The minimum number of samples required to be at a leaf node:\n","'''\n","\n","from sklearn import ensemble\n","\n","params = {'n_estimators': 500, 'max_depth': 3, 'subsample': 0.5,\n","          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n","clf = ensemble.GradientBoostingClassifier(**params)\n","clf.fit(xtrain, ytrain) #trains\n","y_pred = clf.predict(xtest)  #predicts\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, y_pred))\n","print('classification report')\n","print(metrics.classification_report(ytest, y_pred))\n","print(\"-----------------------------------------------------------------------------------------\")\n","print(\"Accuracy is :\")\n","print(metrics.accuracy_score(ytest, y_pred))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))"]},{"cell_type":"markdown","metadata":{},"source":["# 10. Use Extreme Gradient Boosting Alogrithm\n","#### A Brief about Extreme Gradient Boosting Algorithm\n","XGBoost is one of the implementations of Gradient Boosting concept, but what makes XGBoost unique is that it uses “a more regularized model formalization to control over-fitting, which gives it better performance,” according to the author of the algorithm, Tianqi Chen. Therefore, it helps to reduce overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from xgboost.sklearn import XGBClassifier\n","\n","# Create the XGB classifier, xgb_model.\n","xgb_model = XGBClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# List the default parameters.\n","print(xgb_model.get_xgb_params())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train and evaluate.\n","xgb_model.fit(xtrain, ytrain, eval_metric=['error'], eval_set=[((xtrain, ytrain)),(xtest, ytest)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import metrics\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","y_pred = xgb_model.predict(xtest)  #predicts\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, y_pred))\n","print('classification report')\n","print(metrics.classification_report(ytest, y_pred))\n","print(\"-----------------------------------------------------------------------------------------\")\n","print(\"Accuracy is :\")\n","print(metrics.accuracy_score(ytest, y_pred))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))"]},{"cell_type":"markdown","metadata":{},"source":["#### As we can see, the F1 score and Area under the curve are higher compared to previous model. "]},{"cell_type":"markdown","metadata":{},"source":["# 11. Select random variables for model building\n","#### This is done to check whether we can get a lift on the accuracy with fewer variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split The Data with few variables\n","\n","from sklearn.model_selection import train_test_split\n","\n","x = df[['V9', 'V10','V16', 'V17', 'V18', 'Amount']]\n","y = df['Class']\n","\n","xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.30, random_state=0)\n","print('xtrain shape')\n","print(xtrain.shape)\n","print('xtest shape')\n","print(xtest.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Random Forest Classifier on reduced dimensions data'''\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","\n","rfmodel = RandomForestClassifier()\n","rfmodel.fit(xtrain,ytrain)\n","print('model')\n","print(rfmodel)\n","\n","ypredrf = rfmodel.predict(xtest)\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, ypredrf))\n","print('classification report')\n","print(metrics.classification_report(ytest, ypredrf))\n","print('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))"]},{"cell_type":"markdown","metadata":{},"source":["#### The F1 score has reduced with fewer variables along with Area under the curve"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Gradient Boost Algorithm on reduced dimensions data'''\n","\n","from sklearn import ensemble\n","\n","params = {'n_estimators': 500, 'max_depth': 3, 'subsample': 0.5,\n","          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n","clf = ensemble.GradientBoostingClassifier(**params)\n","clf.fit(xtrain, ytrain) #trains\n","y_pred = clf.predict(xtest)  #predicts\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, y_pred))\n","print('classification report')\n","print(metrics.classification_report(ytest, y_pred))\n","print(\"-----------------------------------------------------------------------------------------\")\n","print(\"Accuracy is :\")\n","print(metrics.accuracy_score(ytest, y_pred))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))"]},{"cell_type":"markdown","metadata":{},"source":["#### Reduction in the F1 score with fewer variables."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Extreme Gradient Boost Algorithm on reduced dimensions data'''\n","\n","from xgboost.sklearn import XGBClassifier\n","\n","# Create the XGB classifier, xgb_model.\n","xgb_model = XGBClassifier()\n","# List the default parameters.\n","print(xgb_model.get_xgb_params())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train and evaluate.\n","xgb_model.fit(xtrain, ytrain, eval_metric=['error'], eval_set=[((xtrain, ytrain)),(xtest, ytest)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import metrics\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","y_pred = xgb_model.predict(xtest)  #predicts\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, y_pred))\n","print('classification report')\n","print(metrics.classification_report(ytest, y_pred))\n","print(\"-----------------------------------------------------------------------------------------\")\n","print(\"Accuracy is :\")\n","print(metrics.accuracy_score(ytest, y_pred))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))"]},{"cell_type":"markdown","metadata":{},"source":["#### Higher F1 score compared to Gradient Boosting Model"]},{"cell_type":"markdown","metadata":{},"source":["# 11. Random Under Sampling\n","#### This is done to reduce the imbalance between frauds & non frauds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''We are taking 10% of non frauds and merge it with frauds data'''\n","\n","df = df[['Class', 'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n","       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n","       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\n","\n","\n","sample = df[df['Class']==0]\n","fraud = df[df['Class'] == 1]\n","\n","# random sampling\n","ignore_me, sample = train_test_split(sample, test_size = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","sample = pd.concat([sample, fraud])\n","\n","# Split into train and test units.\n","xtrain, xtest = train_test_split(sample, test_size = 0.3)\n","ytrain = xtrain['Class']\n","ytest = xtest['Class']\n","xtrain.drop('Class', 1, inplace = True)\n","xtest.drop('Class', 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Random Forest Classifier on undersampled data'''\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","\n","rfmodel = RandomForestClassifier()\n","rfmodel.fit(xtrain,ytrain)\n","print('model')\n","print(rfmodel)\n","\n","ypredrf = rfmodel.predict(xtest)\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, ypredrf))\n","print('classification report')\n","print(metrics.classification_report(ytest, ypredrf))\n","print('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))"]},{"cell_type":"markdown","metadata":{},"source":["#### we can see an increase in F1 score which indicates that random under sample can give a lift in the accuracy. However we need to do random sampling multiple times for training the model and then use it on the new data for better results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Gradient Boost Algorithm on undersampled data'''\n","\n","from sklearn import ensemble\n","\n","params = {'n_estimators': 650, 'max_depth': 3, 'subsample': 0.5,\n","          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n","clf = ensemble.GradientBoostingClassifier(**params)\n","clf.fit(xtrain, ytrain) #trains\n","y_pred = clf.predict(xtest)  #predicts\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, y_pred))\n","print('classification report')\n","print(metrics.classification_report(ytest, y_pred))\n","print(\"-----------------------------------------------------------------------------------------\")\n","print(\"Accuracy is :\")\n","print(metrics.accuracy_score(ytest, y_pred))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))"]},{"cell_type":"markdown","metadata":{},"source":["#### There is an increase in the F1 score for this sampling technique with 10% data of non-frauds and 100% frauds. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Extreme Gradient Boost Algorithm on reduced dimensions data'''\n","\n","from xgboost.sklearn import XGBClassifier\n","\n","# Create the XGB classifier, xgb_model.\n","xgb_model = XGBClassifier()\n","\n","# List the default parameters.\n","print(xgb_model.get_xgb_params())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train and evaluate.\n","xgb_model.fit(xtrain, ytrain, eval_metric=['error'], eval_set=[((xtrain, ytrain)),(xtest, ytest)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import metrics\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","y_pred = xgb_model.predict(xtest)  #predicts\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, y_pred))\n","print('classification report')\n","print(metrics.classification_report(ytest, y_pred))\n","print(\"-----------------------------------------------------------------------------------------\")\n","print(\"Accuracy is :\")\n","print(metrics.accuracy_score(ytest, y_pred))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))"]},{"cell_type":"markdown","metadata":{},"source":["#### Higher F1 score comapred to Gradient Boosting Model."]},{"cell_type":"markdown","metadata":{},"source":["# 12. SMOTE ( Synthetic Minority Over-sampling Technique)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install imblearn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import Counter\n","from imblearn.over_sampling import SMOTE \n","\n","x = df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n","       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n","       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\n","y = df['Class']\n","\n","'''Increase the fraud samples from 102 to 500'''\n","\n","sm = SMOTE(random_state=42,sampling_strategy={1:500})\n","X_res, y_res = sm.fit_sample(x, y)\n","print('Resampled dataset shape {}'.format(Counter(y_res)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Split the resampled data into train & test data with 70:30 mix'''\n","\n","xtrain, xtest, ytrain, ytest = train_test_split(X_res, y_res, test_size=0.30, random_state=0)\n","print('xtrain shape')\n","print(xtrain.shape)\n","print('xtest shape')\n","print(xtest.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Random Forest Classifier on resampled data'''\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","\n","rfmodel = RandomForestClassifier()\n","rfmodel.fit(xtrain,ytrain)\n","print('model')\n","print(rfmodel)\n","\n","ypredrf = rfmodel.predict(xtest)\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, ypredrf))\n","print('classification report')\n","print(metrics.classification_report(ytest, ypredrf))\n","print('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))"]},{"cell_type":"markdown","metadata":{},"source":["#### We can see an increase in F1 score and the Area under the curve. This is the advantage of synthetic over sampling technique which has given a good lift in accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Gradient Boost Algorithm on resampled data'''\n","\n","from sklearn import ensemble\n","\n","params = {'n_estimators': 500, 'max_depth': 3, 'subsample': 0.5,\n","          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n","clf = ensemble.GradientBoostingClassifier(**params)\n","clf.fit(xtrain, ytrain) #trains\n","y_pred = clf.predict(xtest)  #predicts\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, y_pred))\n","print('classification report')\n","print(metrics.classification_report(ytest, y_pred))\n","print(\"-----------------------------------------------------------------------------------------\")\n","print(\"Accuracy is :\")\n","print(metrics.accuracy_score(ytest, y_pred))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))"]},{"cell_type":"markdown","metadata":{},"source":["#### There's an increase in precision, F1 score & Area under the curve using SMOTE and resampling the data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''Extreme Gradient Boost Algorithm on reduced dimensions data'''\n","\n","from xgboost.sklearn import XGBClassifier\n","\n","# Create the XGB classifier, xgb_model.\n","xgb_model = XGBClassifier()\n","\n","# List the default parameters.\n","print(xgb_model.get_xgb_params())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train and evaluate.\n","xgb_model.fit(xtrain, ytrain, eval_metric=['error'], eval_set=[((xtrain, ytrain)),(xtest, ytest)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import metrics\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","y_pred = xgb_model.predict(xtest)  #predicts\n","print('confusion matrix')\n","print(metrics.confusion_matrix(ytest, y_pred))\n","print('classification report')\n","print(metrics.classification_report(ytest, y_pred))\n","print(\"-----------------------------------------------------------------------------------------\")\n","print(\"Accuracy is :\")\n","print(metrics.accuracy_score(ytest, y_pred))\n","print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))"]},{"cell_type":"markdown","metadata":{},"source":["#### XGBoost has given high F1 score when compared with Gradient Boosting Model"]},{"cell_type":"markdown","metadata":{},"source":["### We can conclude stating that sampling techniques are important and can be helpful if the data is skewed. We have seen two statistical algorithms (Bagging & Boosting) and their implementation on different samples of data providing different outcomes. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":1}
